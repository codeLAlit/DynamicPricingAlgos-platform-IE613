# -*- coding: utf-8 -*-
"""Dynamic Pricing Using UCB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r6J0a2iUt-gbs3YScqw7pUFBc2olvaL0
"""

import numpy as np
import matplotlib.pyplot as plt
import tqdm as tqdm
from scipy.stats import norm

umaxs = [0.5, 0.1, 0.05, 0.01, 0.005, 0.001]

"""# Dynamic Pricing Using UCB1-P"""

def get_mean(u_max, price):
    return u_max*(1-(0.8*(norm.cdf(price,0.85,0.38**0.5)-norm.cdf(0,0.85,0.38**0.5))+0.2*(norm.cdf(price,3.4,0.05**0.5)-norm.cdf(0,3.4,0.05**0.5))))

def ucbp(time, umax, price, arms=4):
    # assert(price.shape==(1,arms))  
    means = get_mean(umax, price)
    ucb_t = np.zeros(arms)
    X = np.zeros(arms)
    first_term = 0
    reward = 0
    regret = np.zeros(time)
    revenue = np.zeros(time)
    count = 0    
    num_turns = np.ones(arms)
    for i in range(arms):
        X[i] = np.random.binomial(1, means[i], 1)
        revenue[i]=X[i]*price[i]
        reward += revenue[i]
        regret[i] = -1*reward
        ucb_t[i] = X[i] + np.sqrt(4*umax*np.log(i+1))
        
    for t in tqdm.tqdm(range(arms, time)):
        arm = 0
        arm = np.argmax(ucb_t*price)
        
        reward_t = np.random.binomial(1, means[arm], 1)
        revenue[t]=reward_t*price[arm]
        reward += revenue[t]
        num_turns[arm] = num_turns[arm] + 1
        X[arm] = X[arm]*(1.0-1.0/(num_turns[arm])) + reward_t/num_turns[arm]
        
        ucb_t[arm] = X[arm] + np.sqrt(4*umax*np.log(t+1)/num_turns[arm])
        regret[t] = -1*reward
    regret = regret + np.arange(1, time+1)*np.max(price*means)
    return regret, revenue


"""# Dynamic Pricing Using UCB1-O"""

def ucbo(time, umax, price, arms=4):
    # assert(price.shape==(1,arms))  
    means = get_mean(umax, price)
    arm_reward = np.zeros(arms) # remember to update
    ucb_t = np.zeros(arms)
    X = np.zeros((arms,arms))
    T = np.zeros((arms,arms))
    UCB = time*np.ones(arms)
    first_term = 0
    reward = 0
    regret = np.zeros(time)
    revenue = np.zeros(time)
    count = 0    
    num_turns = np.ones(arms)
    for i in range(arms):
        reward_t = np.random.binomial(1, means[i], 1)
        arm_reward[i] += reward_t
        revenue[i]=reward_t*price[i]
        reward += revenue[i]
        regret[i] = -1*reward
        
    for t in tqdm.tqdm(range(arms, time)):
        T = np.ones((arms,arms))
        X = np.zeros((arms,arms))
        UCB = time*np.ones(arms)
        for i in range(arms):
            T[i,i] = num_turns[i]
            X[i,i] = arm_reward[i]/T[i,i]
            for j in reversed(range(i)):
                T[i,j] = T[i,j+1] + num_turns[j]
                X[i,j] = (X[i,j+1]*T[i,j+1] + arm_reward[j])/T[i,j]
            temp = X[i,:] + np.sqrt(2*np.log(t+1)/T[i,:])
            temp.reshape((arms,))
            UCB[i] = min(temp[0:i+1])
        arm = 0
        arm = np.argmax(UCB*price)

        reward_t = np.random.binomial(1, means[arm], 1)
        arm_reward[arm] += reward_t
        revenue[t]=reward_t*price[arm]
        reward += revenue[t]
        num_turns[arm] = num_turns[arm] + 1
        regret[t] = -1*reward
    regret = regret + np.arange(1, time+1)*np.max(price*means)
    return regret, revenue

def ucbpo(time, umax, price, arms=4):
    # assert(price.shape==(1,arms))  
    means = get_mean(umax, price)
    arm_reward = np.zeros(arms) # remember to update
    ucb_t = np.zeros(arms)
    X = np.zeros((arms,arms))
    T = np.zeros((arms,arms))
    UCB = time*np.ones(arms)
    first_term = 0
    reward = 0
    regret = np.zeros(time)
    revenue = np.zeros(time)
    count = 0    
    num_turns = np.ones(arms)
    for i in range(arms):
        reward_t = np.random.binomial(1, means[i], 1)
        arm_reward[i] += reward_t
        revenue[i]=reward_t*price[i]
        reward += revenue[i]
        regret[i] = -1*reward
        
    for t in tqdm.tqdm(range(arms, time)):
        T = np.ones((arms,arms))
        X = np.zeros((arms,arms))
        UCB = time*np.ones(arms)
        for i in range(arms):
            T[i,i] = num_turns[i]
            X[i,i] = arm_reward[i]/T[i,i]
            for j in reversed(range(i)):
                T[i,j] = T[i,j+1] + num_turns[j]
                X[i,j] = (X[i,j+1]*T[i,j+1] + arm_reward[j])/T[i,j]
            temp = X[i,:] + np.sqrt(4*umax*np.log(t+1)/T[i,:])
            temp.reshape((arms,))
            UCB[i] = min(temp[0:i+1])
        arm = 0
        arm = np.argmax(UCB*price)

        reward_t = np.random.binomial(1, means[arm], 1)
        arm_reward[arm] += reward_t
        revenue[t]=reward_t*price[arm]
        reward += revenue[t]
        num_turns[arm] = num_turns[arm] + 1
        regret[t] = -1*reward
    regret = regret + np.arange(1, time+1)*np.max(price*means)
    return regret, revenue
